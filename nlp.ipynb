{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        RT @GOPChairwoman: House Democrats are engaged...\n",
       "1        RT @rww_gop: Voters are rejecting the Dems’ ba...\n",
       "2        As the Witch Hunt continues! https://t.co/klvv...\n",
       "3        The Greatest Witch Hunt in American History! h...\n",
       "4        RT @realDonaldTrump: Years of BAD GOVERNMENT, ...\n",
       "                               ...                        \n",
       "11407    The forgotten men and women of our country wil...\n",
       "11408    January 20th 2017, will be remembered as the d...\n",
       "11409    What truly matters is not which party controls...\n",
       "11410    power from Washington, D.C. and giving it back...\n",
       "11411    Today we are not merely transferring power fro...\n",
       "Name: text, Length: 11412, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re,string,nltk,csv\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize , TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "import datetime\n",
    "\n",
    "\n",
    "    \n",
    "data = pd.read_csv(\"/Users/keshavgaddhyan/Desktop/NLP project/trump_tweets.csv\")\n",
    "tknzr = TweetTokenizer()\n",
    "wnl=WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english') + ['...']\n",
    "punctuation=set(string.punctuation)\n",
    "data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[rt, @gopchairwoman, house, democrat, engaged,...</td>\n",
       "      <td>10-17-2019 15:09:09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[rt, @rww_gop, voter, rejecting, dems, ’, base...</td>\n",
       "      <td>10-17-2019 15:08:35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[witch, hunt, continues]</td>\n",
       "      <td>10-17-2019 15:07:46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[greatest, witch, hunt, american, history]</td>\n",
       "      <td>10-17-2019 13:14:49</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[rt, @realdonaldtrump, year, bad, government, ...</td>\n",
       "      <td>10-17-2019 12:59:25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           created_at  \\\n",
       "0  [rt, @gopchairwoman, house, democrat, engaged,...  10-17-2019 15:09:09   \n",
       "1  [rt, @rww_gop, voter, rejecting, dems, ’, base...  10-17-2019 15:08:35   \n",
       "2                           [witch, hunt, continues]  10-17-2019 15:07:46   \n",
       "3         [greatest, witch, hunt, american, history]  10-17-2019 13:14:49   \n",
       "4  [rt, @realdonaldtrump, year, bad, government, ...  10-17-2019 12:59:25   \n",
       "\n",
       "  Unnamed: 2  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_tweet(tweet):\n",
    "        tweet=tweet.lower()          # converts the tweets tro lower case\n",
    "        tweet=re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet) # removes links in the tweets\n",
    "        tweet=tknzr.tokenize(tweet)              #tokenize the tweets into words\n",
    "        new_tweet=[]\n",
    "        for word in tweet:\n",
    "            word=wnl.lemmatize(word) #lemmatize the words\n",
    "            if word not in (stop_words) and word not in punctuation: # remove stop words and punctuations from the tweets\n",
    "                new_tweet.append(word)\n",
    "        return new_tweet\n",
    "for row in data.iterrows():\n",
    "    row[1][\"text\"]=clean_tweet(row[1][\"text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.044*\"bill\" + 0.033*\"hurricane\" + 0.031*\"care\" + 0.028*\"yesterday\" + 0.028*\"c\" + 0.020*\"signed\" + 0.019*\"thanks\" + 0.019*\"local\" + 0.018*\"sign\" + 0.018*\"support\"\n",
      "Topic: 1 \n",
      "Words: 0.059*\"looking\" + 0.041*\"@flotus\" + 0.036*\"god\" + 0.034*\"friend\" + 0.033*\"south\" + 0.032*\"wonderful\" + 0.027*\"action\" + 0.026*\"carolina\" + 0.025*\"japan\" + 0.024*\"truly\"\n",
      "Topic: 2 \n",
      "Words: 0.046*\"u\" + 0.032*\"deal\" + 0.031*\"year\" + 0.025*\"trade\" + 0.024*\"china\" + 0.021*\"ha\" + 0.021*\"many\" + 0.021*\"country\" + 0.019*\"billion\" + 0.019*\"dollar\"\n",
      "Topic: 3 \n",
      "Words: 0.111*\"thank\" + 0.055*\"working\" + 0.053*\"hard\" + 0.042*\"great\" + 0.031*\"night\" + 0.027*\"last\" + 0.026*\"big\" + 0.020*\"together\" + 0.019*\"rally\" + 0.018*\"see\"\n",
      "Topic: 4 \n",
      "Words: 0.043*\"job\" + 0.042*\"ever\" + 0.035*\"history\" + 0.033*\"economy\" + 0.028*\"number\" + 0.027*\"since\" + 0.027*\"year\" + 0.027*\"best\" + 0.026*\"record\" + 0.024*\"market\"\n",
      "Topic: 5 \n",
      "Words: 0.048*\"watch\" + 0.031*\"build\" + 0.025*\"interview\" + 0.025*\"confidence\" + 0.023*\"leadership\" + 0.021*\"v\" + 0.020*\"school\" + 0.020*\"crisis\" + 0.017*\"nfl\" + 0.016*\"easy\"\n",
      "Topic: 6 \n",
      "Words: 0.086*\"border\" + 0.069*\"security\" + 0.062*\"wall\" + 0.038*\"national\" + 0.030*\"democrat\" + 0.022*\"southern\" + 0.019*\"prayer\" + 0.018*\"use\" + 0.017*\"thought\" + 0.015*\"including\"\n",
      "Topic: 7 \n",
      "Words: 0.066*\"fbi\" + 0.040*\"comey\" + 0.034*\"’\" + 0.026*\"@potus\" + 0.026*\"james\" + 0.024*\"russia\" + 0.022*\"investigation\" + 0.017*\"brave\" + 0.013*\"director\" + 0.013*\"report\"\n",
      "Topic: 8 \n",
      "Words: 0.106*\"house\" + 0.060*\"white\" + 0.038*\"prime\" + 0.038*\"minister\" + 0.034*\"healthcare\" + 0.025*\"victory\" + 0.023*\"obstruction\" + 0.020*\"proud\" + 0.015*\"representative\" + 0.014*\"time\"\n",
      "Topic: 9 \n",
      "Words: 0.073*\"meeting\" + 0.066*\"korea\" + 0.062*\"north\" + 0.047*\"look\" + 0.033*\"forward\" + 0.028*\"president\" + 0.027*\"special\" + 0.021*\"kim\" + 0.018*\"relationship\" + 0.017*\"good\"\n",
      "Topic: 10 \n",
      "Words: 0.034*\"florida\" + 0.028*\"already\" + 0.026*\"book\" + 0.025*\"france\" + 0.023*\"start\" + 0.022*\"great\" + 0.019*\"lot\" + 0.017*\"dem\" + 0.016*\"early\" + 0.016*\"mainstream\"\n",
      "Topic: 11 \n",
      "Words: 0.077*\"vote\" + 0.071*\"republican\" + 0.048*\"win\" + 0.043*\"senate\" + 0.030*\"big\" + 0.026*\"get\" + 0.024*\"’\" + 0.018*\"democrat\" + 0.016*\"let\" + 0.015*\"bill\"\n",
      "Topic: 12 \n",
      "Words: 0.266*\"rt\" + 0.205*\"…\" + 0.075*\"@realdonaldtrump\" + 0.044*\"trump\" + 0.039*\"president\" + 0.025*\"@whitehouse\" + 0.017*\"american\" + 0.011*\"live\" + 0.011*\"dnc\" + 0.010*\"today\"\n",
      "Topic: 13 \n",
      "Words: 0.033*\"must\" + 0.031*\"want\" + 0.028*\"country\" + 0.027*\"people\" + 0.027*\"democrat\" + 0.021*\"’\" + 0.019*\"immigration\" + 0.017*\"stop\" + 0.017*\"border\" + 0.017*\"mexico\"\n",
      "Topic: 14 \n",
      "Words: 0.036*\"general\" + 0.036*\"new\" + 0.029*\"failing\" + 0.028*\"mueller\" + 0.025*\"york\" + 0.023*\"enjoy\" + 0.021*\"@foxandfriends\" + 0.021*\"@nytimes\" + 0.018*\"morning\" + 0.018*\"lawyer\"\n",
      "Topic: 15 \n",
      "Words: 0.073*\"wa\" + 0.051*\"collusion\" + 0.037*\"russian\" + 0.030*\"election\" + 0.017*\"press\" + 0.016*\"hoax\" + 0.015*\"angry\" + 0.014*\"information\" + 0.014*\"dossier\" + 0.014*\"virginia\"\n",
      "Topic: 16 \n",
      "Words: 0.044*\"great\" + 0.035*\"woman\" + 0.026*\"thank\" + 0.025*\"law\" + 0.023*\"first\" + 0.022*\"america\" + 0.022*\"stand\" + 0.021*\"men\" + 0.020*\"texas\" + 0.018*\"people\"\n",
      "Topic: 17 \n",
      "Words: 0.134*\"great\" + 0.084*\"america\" + 0.053*\"make\" + 0.046*\"day\" + 0.024*\"thing\" + 0.023*\"people\" + 0.022*\"country\" + 0.015*\"beautiful\" + 0.014*\"success\" + 0.013*\"work\"\n",
      "Topic: 18 \n",
      "Words: 0.042*\"p\" + 0.041*\"@foxnews\" + 0.034*\"tonight\" + 0.024*\"interviewed\" + 0.023*\"presidential\" + 0.020*\"@seanhannity\" + 0.019*\"@ivankatrump\" + 0.019*\"next\" + 0.018*\"american\" + 0.018*\"drug\"\n",
      "Topic: 19 \n",
      "Words: 0.094*\"#maga\" + 0.059*\"join\" + 0.048*\"cnn\" + 0.034*\"@scavino45\" + 0.027*\"michael\" + 0.022*\"way\" + 0.019*\"rating\" + 0.018*\"steve\" + 0.016*\"freedom\" + 0.015*\"room\"\n",
      "Topic: 20 \n",
      "Words: 0.054*\"party\" + 0.041*\"pay\" + 0.033*\"pelosi\" + 0.033*\"heading\" + 0.032*\"highest\" + 0.030*\"approval\" + 0.028*\"nancy\" + 0.023*\"democrat\" + 0.022*\"republican\" + 0.020*\"nothing\"\n",
      "Topic: 21 \n",
      "Words: 0.136*\"state\" + 0.088*\"united\" + 0.050*\"congratulation\" + 0.044*\"nation\" + 0.034*\"great\" + 0.023*\"senator\" + 0.016*\"repeal\" + 0.014*\"replace\" + 0.013*\"people\" + 0.013*\"world\"\n",
      "Topic: 22 \n",
      "Words: 0.064*\"wa\" + 0.063*\"honor\" + 0.047*\"great\" + 0.040*\"today\" + 0.030*\"welcome\" + 0.027*\"president\" + 0.023*\"@whitehouse\" + 0.022*\"john\" + 0.017*\"congressman\" + 0.013*\"respected\"\n",
      "Topic: 23 \n",
      "Words: 0.039*\"place\" + 0.032*\"office\" + 0.032*\"american\" + 0.026*\"hero\" + 0.025*\"took\" + 0.024*\"daca\" + 0.024*\"administration\" + 0.021*\"million\" + 0.018*\"force\" + 0.018*\"energy\"\n",
      "Topic: 24 \n",
      "Words: 0.109*\"tax\" + 0.065*\"cut\" + 0.022*\"farmer\" + 0.016*\"level\" + 0.016*\"disaster\" + 0.015*\"regulation\" + 0.014*\"would\" + 0.014*\"big\" + 0.014*\"much\" + 0.014*\"rico\"\n",
      "Topic: 25 \n",
      "Words: 0.064*\"hunt\" + 0.064*\"witch\" + 0.062*\"hillary\" + 0.061*\"clinton\" + 0.048*\"campaign\" + 0.041*\"crooked\" + 0.039*\"wa\" + 0.036*\"obama\" + 0.020*\"gave\" + 0.018*\"pennsylvania\"\n",
      "Topic: 26 \n",
      "Words: 0.036*\"court\" + 0.033*\"justice\" + 0.029*\"w\" + 0.023*\"life\" + 0.020*\"washington\" + 0.018*\"supreme\" + 0.017*\"american\" + 0.016*\"family\" + 0.016*\"judge\" + 0.015*\"post\"\n",
      "Topic: 27 \n",
      "Words: 0.064*\"“\" + 0.062*\"”\" + 0.051*\"’\" + 0.045*\"news\" + 0.040*\"fake\" + 0.027*\"medium\" + 0.024*\"trump\" + 0.018*\"president\" + 0.015*\"wa\" + 0.014*\"people\"\n",
      "Topic: 28 \n",
      "Words: 0.062*\"story\" + 0.038*\"never\" + 0.024*\"wa\" + 0.016*\"dishonest\" + 0.016*\"phony\" + 0.016*\"another\" + 0.015*\"illegal\" + 0.014*\"watching\" + 0.014*\"public\" + 0.014*\"like\"\n",
      "Topic: 29 \n",
      "Words: 0.072*\"job\" + 0.055*\"ha\" + 0.053*\"great\" + 0.051*\"military\" + 0.043*\"crime\" + 0.038*\"strong\" + 0.033*\"love\" + 0.027*\"total\" + 0.026*\"done\" + 0.025*\"full\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(data[\"text\"])\n",
    "# print(dictionary)\n",
    "# mycorpus is a list of list|\n",
    "# where each list is a tweet and it is a list of tuples [(token_id, no_of_times_it_occurs_in_tweet),(),...]\n",
    "mycorpus = [dictionary.doc2bow(tweet, allow_update=True) for tweet in data[\"text\"]]\n",
    "\n",
    "word_counts = [[(dictionary[id], count) for id, count in line] for line in mycorpus]\n",
    "\n",
    "# lda_model = LdaModel(corpus=mycorpus,id2word=dictionary, num_topics=30,passes=10)\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=mycorpus,id2word=dictionary,\n",
    "num_topics=30, passes = 10)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_tweets=[\"trade\", \"currency\", \"economy\", \"growth\",  \"trade-war\", \"rates\", \"inflation\", \"manipulation\", \"dollar\", \"Fed\", \"Powell\", \"tariffs\", ]\n",
    "for i in range(len(trade_tweets)):\n",
    "    trade_tweets[i]=wnl.lemmatize(trade_tweets[i])  #lemmatize the words in our bag of words for trade realted tweets as well so that it is coherant with our lemmatized tweeets\n",
    "def istrade_tweet(tweet):\n",
    "    for word in trade_tweets: \n",
    "        if word in tweet:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "        \n",
    "        \n",
    "            \n",
    "for index,row in data.iterrows():\n",
    "    if (istrade_tweet(row[\"text\"]))==False:\n",
    "        data.drop(index, inplace=True)\n",
    "        \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(1,\"Sentiment\",\"Positive\")\n",
    "def analyze_sentiment(tweet):\n",
    "    listToStr = ' '.join(map(str, tweet))\n",
    "    analysis = TextBlob(listToStr)\n",
    "    if analysis.sentiment.polarity > 0.7:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity < -0.7:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "for index,row in data.iterrows():\n",
    "    row[\"Sentiment\"]=analyze_sentiment(row[\"text\"])\n",
    "    \n",
    "data\n",
    "# data.to_csv(r'/Users/keshavgaddhyan/Desktop/NLP project/sentiment.csv')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500=pd.read_csv(\"/Users/keshavgaddhyan/Desktop/NLP project/yahoo finance s&p500.csv\")\n",
    "# sp500.drop([\"High\", \"Low\",\"Adj Close\",\"Volume\"], axis = 0) \n",
    "\n",
    "# x axis values \n",
    "# x = [1,2,3] \n",
    "# # corresponding y axis values \n",
    "# y = [2,4,1] \n",
    "figure(num=None, figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\n",
    "sp500['Date'] = pd.to_datetime(sp500['Date'])\n",
    "data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "sp500.set_index('Date')['Close'].plot();\n",
    "\n",
    "\n",
    "# plotting the points\n",
    "# plt.plot(sp500[\"Date\"], sp500[\"Close\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,row in data.iterrows():\n",
    "    if row[\"Sentiment\"]==\"Neutral\":\n",
    "        data.drop(index, inplace=True)\n",
    "\n",
    "sp500.insert(1,\"Portfolio\",0)\n",
    "sp500.insert(1,\"Units\",0)\n",
    "sp500.insert(1,\"Cash\",0)\n",
    "sp500.loc[0,\"Units\"]=1\n",
    "sp500.loc[0,\"Portfolio\"]=2269.959961\n",
    "\n",
    "data\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for index,row in sp500.iterrows():\n",
    "#     print(row[\"Date\"])\n",
    "prevunit=1\n",
    "prevcash=0\n",
    "prevport=0\n",
    "action=\"none\"\n",
    "opening_time=datetime.time(9,30,0)\n",
    "closing_time=datetime.time(16,30,0)\n",
    "for index,row in sp500.iterrows():\n",
    "    count=0\n",
    "    for idx,rows in data.iterrows():  \n",
    "        if (row[\"Date\"].date())==(rows[\"created_at\"].date()):\n",
    "            \n",
    "            if rows[\"created_at\"].time() > opening_time and rows[\"created_at\"].time() < closing_time:\n",
    "                print(rows[\"text\"])\n",
    "                if rows[\"Sentiment\"]==\"Positive\":\n",
    "                    units_bought=2000/row[\"Close\"]  #borrowed 2000 at risk free rate\n",
    "                    prevunit=prevunit+units_bought\n",
    "                    sp500.loc[index,\"Units\"]=prevunit\n",
    "                    sp500.loc[index,\"Portfolio\"]=prevunit*row[\"Close\"]\n",
    "                    sp500.loc[index,\"Cash\"]=prevcash\n",
    "                    action=\"bought_by_borrowing\"\n",
    "                    count=1\n",
    "                    \n",
    "                else:\n",
    "                    #we are shorting the stock and also selling what we own\n",
    "#                     row[\"Cash\"]+=row[\"Units\"]*row[\"Close\"]\n",
    "#                     row[\"Cash\"]+=2000\n",
    "                    # shorting for money worth 2000 so adding it temorsrily to cash and then subtract tomorrow\n",
    "\n",
    "                    prevcash=prevcash+(prevunit*row[\"Close\"])+2000\n",
    "                    sp500.loc[index,\"Cash\"]=prevcash\n",
    "                    prevunit=0\n",
    "                    sp500.loc[index,\"Units\"]= prevunit\n",
    "                    action=\"Shorted\"\n",
    "                    sp500.loc[index,\"Portfolio\"]=0\n",
    "                    prevunit=-2000/row[\"Close\"]\n",
    "                    sp500.loc[index,\"Units\"]=prevunit       \n",
    "                    count=1\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            if rows[\"created_at\"].time() < opening_time: \n",
    "                print(rows[\"text\"],rows[\"Sentiment\"])\n",
    "                if rows[\"Sentiment\"]==\"Positive\":   # we borrowed money in the morning and returned at market close, assuming no interest rate for this\n",
    "                    units_bought=2000/row[\"Open\"]\n",
    "                    money_received=units_bought*row[\"Close\"]\n",
    "                    prevcash+=money_received-2000\n",
    "                    sp500.loc[index,\"Units\"]=prevunit\n",
    "                    sp500.loc[index,\"Portfolio\"]=prevunit*row[\"Close\"]\n",
    "                    sp500.loc[index,\"Cash\"]=prevcash\n",
    "                    count=1\n",
    "\n",
    "                else:\n",
    "                    prevcash+=(prevunit*(row[\"Open\"]-row[\"Close\"])) + 2000\n",
    "                    prevcash=prevcash-(2000/row[\"Open\"])*row[\"Close\"] # the shorted amount\n",
    "                    sp500.loc[index,\"Units\"]=prevunit\n",
    "                    sp500.loc[index,\"Portfolio\"]=prevunit*row[\"Close\"]\n",
    "                    sp500.loc[index,\"Cash\"]=prevcash \n",
    "                    count=1\n",
    "                    \n",
    "            if rows[\"created_at\"].time() > closing_time :\n",
    "                print(rows[\"text\"],rows[\"created_at\"],rows[\"Sentiment\"])\n",
    "                if rows[\"Sentiment\"]==\"Positive\":\n",
    "                    action=\"after_hours_positive\"\n",
    "                    count=1\n",
    "                else:\n",
    "                    action=\"after_hours_negative\"\n",
    "                    count=1    \n",
    "                \n",
    "# rows[\"created_at\"].hour >16\n",
    "              \n",
    "   \n",
    "\n",
    "    if count==0:\n",
    "        \n",
    "        if action==\"bought_by_borrowing\":\n",
    "#             row[\"Cash\"]=(row[\"Units\"]-1)*row[\"Close\"]\n",
    "#             row[\"Cash\"]-=2000+(2000*(1.78/100)*(1/365))  # returning the 2k borrowed plus the interest at risk free rate of 1.78 percent\n",
    "#             sp500.loc[index,\"Units\"]=1\n",
    "#             sp500.loc[index,\"Portfolio\"]=row[\"Units\"]*row[\"Close\"]\n",
    "            action=\"none\"\n",
    "            prevcash+=((prevunit-1)*row[\"Close\"])-(2000+(2000*(1.78/100)*(1/365)))  # returning the 2k borrowed plus the interest at risk free rate of 1.78 percent\n",
    "            #we sold everything except 1 unit and then returned the borrowed sum\n",
    "            prevunit=1\n",
    "        elif action==\"Shorted\":\n",
    "            \n",
    "#             row[\"Cash\"]-=(-row[\"Units\"]+1)*row[\"Close\"] #subtracted the shorted units from my cash\n",
    "#             sp500.loc[index,\"Units\"]=1\n",
    "#             sp500.loc[index,\"Portfolio\"]=row[\"Units\"]*row[\"Close\"]\n",
    "            action=\"none\"\n",
    "            prevcash=prevcash-((1-prevunit)*row[\"Close\"]) #subtracted the shorted units from my cash\n",
    "            prevunit=1\n",
    "            \n",
    "        elif action==\"after_hours_positive\":\n",
    "            units_bought=2000/row[\"Open\"]\n",
    "            money_received=units_bought*row[\"Close\"]\n",
    "            print(money_received)\n",
    "            prevcash+=money_received-2000\n",
    "            action=\"none\"\n",
    "            prevunit=1\n",
    "#             sp500.loc[index,\"Units\"]=prevunit\n",
    "#             sp500.loc[index,\"Portfolio\"]=prevunit*row[\"Close\"]\n",
    "#             sp500.loc[index,\"Cash\"]=prevcash\n",
    "            \n",
    "        elif action == \"after_hours_negative\":\n",
    "            prevcash+=(prevunit*(row[\"Open\"]-row[\"Close\"])) + 2000\n",
    "            prevcash=prevcash-((2000/row[\"Open\"])*row[\"Close\"]) # the shorted amount\n",
    "            action=\"none\"\n",
    "            prevunit=1\n",
    "#             sp500.loc[index,\"Units\"]=prevunit\n",
    "#             sp500.loc[index,\"Portfolio\"]=prevunit*row[\"Close\"]\n",
    "#             sp500.loc[index,\"Cash\"]=prevcash \n",
    "            \n",
    "            \n",
    "            \n",
    "               \n",
    "        sp500.loc[index,\"Units\"]=prevunit\n",
    "        sp500.loc[index,\"Portfolio\"]=row[\"Units\"]*row[\"Close\"]\n",
    "        sp500.loc[index,\"Cash\"]=prevcash\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(sp500)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# for index,row in data.iterrows():\n",
    "#     print(row[\"created_at\"].time())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
